{
  "cells": [
    {
      "metadata": {
        "id": "cf4d8c9b006ada45"
      },
      "cell_type": "markdown",
      "source": [
        "## <center>CSE 546: Reinforcement Learning</center>\n",
        "### <center>Prof. Alina Vereshchaka</center>\n",
        "#### <center>Spring 2025</center>\n",
        "\n",
        "Welcome to the Assignment 3, Part 1: Introduction to Actor-Critic Methods! It includes the implementation of simple actor and critic networks and best practices used in modern Actor-Critic algorithms."
      ],
      "id": "cf4d8c9b006ada45"
    },
    {
      "metadata": {
        "id": "9d7a6d891e2fb312"
      },
      "cell_type": "markdown",
      "source": [
        "## Section 0: Setup and Imports"
      ],
      "id": "9d7a6d891e2fb312"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53473293aa9daf8e",
        "outputId": "6f3eb920-765d-4cfe-88ac-4270124f005b"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "\n",
        "# Set seed for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ],
      "id": "53473293aa9daf8e",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7976783b6e50>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "execution_count": 1
    },
    {
      "metadata": {
        "id": "2a3d9c34ff222994"
      },
      "cell_type": "markdown",
      "source": [
        "## Section 1: Actor-Critic Network Architectures and Loss Computation\n",
        "\n",
        "In this section, you will explore two common architectural designs for Actor-Critic methods and implement their corresponding loss functions using dummy tensors. These architectures are:\n",
        "- A. Completely separate actor and critic networks\n",
        "- B. A shared network with two output heads\n",
        "\n",
        "Both designs are widely used in practice. Shared networks are often more efficient and generalize better, while separate networks offer more control and flexibility.\n",
        "\n",
        "---\n"
      ],
      "id": "2a3d9c34ff222994"
    },
    {
      "metadata": {
        "id": "971fa7887dd4f858"
      },
      "cell_type": "markdown",
      "source": [
        "### Task 1a â€“ Separate Actor and Critic Networks with Loss Function\n",
        "\n",
        "Define a class `SeparateActorCritic`. Your goal is to:\n",
        "- Create two completely independent neural networks: one for the actor and one for the critic.\n",
        "- The actor should output a probability distribution over discrete actions (use `nn.Softmax`).\n",
        "- The critic should output a single scalar value.\n",
        "\n",
        " Use `nn.ReLU()` as your activation function. Include at least one hidden layer of reasonable width (e.g. 64 or 128 units).\n",
        "\n",
        "```python\n",
        "# TODO: Define SeparateActorCritic class\n",
        "```\n",
        "\n",
        " Next, simulate training using dummy tensors:\n",
        "1. Generate dummy tensors for log-probabilities, returns, estimated values, and entropies.\n",
        "2. Compute the actor loss using the advantage (return - value).\n",
        "3. Compute the critic loss as mean squared error between values and returns.\n",
        "4. Use a single optimizer for both the Actor and the Critic. In this case, combine the actor and critic losses into a total loss and perform backpropagation.\n",
        "5. Use a separate optimizers for both the Actor and the Critic. In this case, keep the actor and critic losses separate and perform backpropagation.\n",
        "\n",
        "```python\n",
        "# TODO: Simulate loss computation and backpropagation\n",
        "```\n",
        "\n",
        "ðŸ”— Helpful references:\n",
        "- PyTorch Softmax: https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html\n",
        "- PyTorch MSE Loss: https://pytorch.org/docs/stable/generated/torch.nn.functional.mse_loss.html\n",
        "\n",
        "---"
      ],
      "id": "971fa7887dd4f858"
    },
    {
      "metadata": {
        "id": "dd6b81ed1791e4e6"
      },
      "cell_type": "code",
      "source": [
        "# TODO: Define a class SeparateActorCritic with separate networks for actor and critic\n",
        "\n",
        "# BEGIN_YOUR_CODE\n",
        "\n",
        "class SeparateActorCritic(nn.Module):\n",
        "    def __init__(self, stdim, actdim, hiddensiz=128):\n",
        "        super(SeparateActorCritic, self).__init__()\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(stdim, hiddensiz),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hiddensiz, actdim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(stdim, hiddensiz),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hiddensiz, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        actn_p = self.actor(state)\n",
        "        stva = self.critic(state)\n",
        "        return actn_p, stva\n",
        "\n",
        "# END_YOUR_CODE"
      ],
      "id": "dd6b81ed1791e4e6",
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "stdim = 4\n",
        "actdim = 2\n",
        "batchsiz = 5\n",
        "hiddensiz = 128\n",
        "\n",
        "model = SeparateActorCritic(stdim, actdim, hiddensiz)\n",
        "states = torch.randn(batchsiz, stdim)\n",
        "actions = torch.randint(0, actdim, (batchsiz,))\n",
        "returns = torch.randn(batchsiz, 1)\n",
        "actn_p, values = model(states)\n",
        "dist = torch.distributions.Categorical(actn_p)\n",
        "logp = dist.log_prob(actions)\n",
        "entropies = dist.entropy()\n",
        "\n",
        "\n",
        "advantages = returns - values.detach()\n"
      ],
      "metadata": {
        "id": "Gh2vO4OEWdMj"
      },
      "id": "Gh2vO4OEWdMj",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "actoloss = -(logp * advantages.squeeze()).mean()\n",
        "critiloss = F.mse_loss(values, returns)\n",
        "entropyb = entropies.mean()\n",
        "totalloss = actoloss + critiloss - 0.01 * entropyb\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "optimizer.zero_grad()\n",
        "totalloss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "print(\"completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFeou4v3WgIH",
        "outputId": "cbe421ab-311f-45d7-aa52-f08a4bae4fd5"
      },
      "id": "vFeou4v3WgIH",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "actn_p, values = model(states)\n",
        "dist = torch.distributions.Categorical(actn_p)\n",
        "logp = dist.log_prob(actions)\n",
        "entropies = dist.entropy()\n",
        "advantages = returns - values.detach()\n",
        "\n",
        "actoloss = -(logp * advantages.squeeze()).mean()\n",
        "critiloss = F.mse_loss(values, returns)\n",
        "actorparams = list(model.actor.parameters())\n",
        "criticparams = list(model.critic.parameters())\n",
        "actoropti = optim.Adam(actorparams, lr=1e-3)\n",
        "critiopti = optim.Adam(criticparams, lr=1e-3)\n",
        "actoropti.zero_grad()\n",
        "actoloss.backward(retain_graph=True)\n",
        "actoropti.step()\n",
        "critiopti.zero_grad()\n",
        "critiloss.backward()\n",
        "critiopti.step()\n",
        "\n",
        "print(\" separate optimizers completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhrZL4f8Wjcl",
        "outputId": "cbf10e60-2adf-4de6-dfaf-37eee83f4ce2"
      },
      "id": "BhrZL4f8Wjcl",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " separate optimizers completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n",
        "print(\"\\nActor Network:\")\n",
        "print(model.actor)\n",
        "\n",
        "print(\"\\nCritic Network:\")\n",
        "print(model.critic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fA2PVS-wYWi_",
        "outputId": "37974c80-843c-41c3-c82a-18ad5467d972"
      },
      "id": "fA2PVS-wYWi_",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SeparateActorCritic(\n",
            "  (actor): Sequential(\n",
            "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
            "    (3): Softmax(dim=-1)\n",
            "  )\n",
            "  (critic): Sequential(\n",
            "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Actor Network:\n",
            "Sequential(\n",
            "  (0): Linear(in_features=4, out_features=128, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=128, out_features=2, bias=True)\n",
            "  (3): Softmax(dim=-1)\n",
            ")\n",
            "\n",
            "Critic Network:\n",
            "Sequential(\n",
            "  (0): Linear(in_features=4, out_features=128, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=128, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Action Probabilities: \\n{actn_p}\")\n",
        "print(f\"Log Probabilities: \\n{logp}\")\n",
        "print(f\"Actor Loss: {actoloss.item()}\")\n",
        "print(f\"Critic Loss: {critiloss.item()}\")\n",
        "print(f\"Entropy Bonus: {entropyb.item()}\")\n",
        "print(f\"Total Loss (Actor + Critic + Entropy): {totalloss.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SAzE9twYeWT",
        "outputId": "f29e944e-39f9-455d-f50f-f6a727655ec4"
      },
      "id": "3SAzE9twYeWT",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Probabilities: \n",
            "tensor([[0.5427, 0.4573],\n",
            "        [0.4347, 0.5653],\n",
            "        [0.4420, 0.5580],\n",
            "        [0.4095, 0.5905],\n",
            "        [0.3245, 0.6755]], grad_fn=<SoftmaxBackward0>)\n",
            "Log Probabilities: \n",
            "tensor([-0.6113, -0.8332, -0.5833, -0.8927, -0.3924],\n",
            "       grad_fn=<SqueezeBackward1>)\n",
            "Actor Loss: 0.23915183544158936\n",
            "Critic Loss: 1.0585072040557861\n",
            "Entropy Bonus: 0.6783283352851868\n",
            "Total Loss (Actor + Critic + Entropy): 1.365321397781372\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "eb8e90c88108cd2e"
      },
      "cell_type": "markdown",
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:"
      ],
      "id": "eb8e90c88108cd2e"
    },
    {
      "metadata": {
        "id": "98ea382314354335"
      },
      "cell_type": "code",
      "source": [
        "#This setup uses a shared optimizer for both the actor and the critic, by updating both networks simultaneously based on a combined loss.\n",
        "#In the Combined Actor and Critic Losses the actor and critic losses are combined into a single loss.\n",
        "#The critic's loss is calculated using MSE between predicted values and true returns which provides\n",
        "# a stable method for value function approximation.\n",
        "#A single optimizer simplifies training by updating both networks together ideal for environments with closely related actor-critic dynamics.\n",
        "#separate optimizers offer more control, allowing for independent learning rates for the actor and critic which is good for comple environments.\n"
      ],
      "id": "98ea382314354335",
      "outputs": [],
      "execution_count": 8
    },
    {
      "metadata": {
        "id": "64081a606b93029d"
      },
      "cell_type": "markdown",
      "source": [
        "### Task 1b â€“ Shared Network with Actor and Critic Heads + Loss Function\n",
        "\n",
        "Now define a class `SharedActorCritic`:\n",
        "- Build a shared base network (e.g., linear layer + ReLU)\n",
        "- Create two heads: one for actor (output action probabilities) and one for critic (output state value)\n",
        "\n",
        "```python\n",
        "# TODO: Define SharedActorCritic class\n",
        "```\n",
        "\n",
        "Then:\n",
        "1. Pass a dummy input tensor through the model to obtain action probabilities and value.\n",
        "2. Simulate dummy rewards and compute advantage.\n",
        "3. Compute the actor and critic losses, combine them, and backpropagate.\n",
        "\n",
        "```python\n",
        "# TODO: Simulate shared network loss computation and backpropagation\n",
        "```\n",
        "\n",
        " Use `nn.Softmax` for actor output and `nn.Linear` for scalar critic output.\n",
        "\n",
        "ðŸ”— More reading:\n",
        "- Policy Gradient Methods: https://spinningup.openai.com/en/latest/algorithms/vpg.html\n",
        "- Actor-Critic Overview: https://www.tensorflow.org/agents/tutorials/6_reinforce_tutorial\n",
        "- PyTorch Categorical Distribution: https://pytorch.org/docs/stable/distributions.html#categorical\n",
        "\n",
        "---"
      ],
      "id": "64081a606b93029d"
    },
    {
      "metadata": {
        "id": "a48f882fff11aecc"
      },
      "cell_type": "code",
      "source": [
        "# BEGIN_YOUR_CODE\n",
        "class SharedActorCritic(nn.Module):\n",
        "    def __init__(self, stdim, actdim, hiddensiz=128):\n",
        "        super(SharedActorCritic, self).__init__()\n",
        "\n",
        "        self.shared_base = nn.Sequential(\n",
        "            nn.Linear(stdim, hiddensiz),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.actor_head = nn.Linear(hiddensiz, actdim)\n",
        "        self.critic_head = nn.Linear(hiddensiz, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        so = self.shared_base(state)\n",
        "        actn_p = torch.softmax(self.actor_head(so), dim=-1)\n",
        "        stva = self.critic_head(so)\n",
        "        return actn_p, stva\n",
        "\n",
        "\n",
        "# END_YOUR_CODE"
      ],
      "id": "a48f882fff11aecc",
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "stdim = 4\n",
        "actdim = 2\n",
        "batchsiz = 5\n",
        "hiddensiz = 128\n",
        "model = SharedActorCritic(stdim, actdim, hiddensiz)\n",
        "states = torch.randn(batchsiz, stdim)\n",
        "actions = torch.randint(0, actdim, (batchsiz,))\n",
        "returns = torch.randn(batchsiz, 1)\n",
        "actn_p, values = model(states)\n",
        "dist = torch.distributions.Categorical(actn_p)\n",
        "logp = dist.log_prob(actions)\n",
        "entropies = dist.entropy()\n",
        "advantages = returns - values.detach()\n",
        "actoloss = -(logp * advantages.squeeze()).mean()\n",
        "critiloss = F.mse_loss(values, returns)\n",
        "entropyb = entropies.mean()\n",
        "totalloss = actoloss + critiloss - 0.01 * entropyb\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "optimizer.zero_grad()\n",
        "totalloss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "print(\"Shared network loss computation and backpropagation completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zpnuu6vJbxLF",
        "outputId": "60117cd1-2b9e-4e6f-9183-84c564ad3ce6"
      },
      "id": "Zpnuu6vJbxLF",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shared network loss computation and backpropagation completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Actor Loss:\", actoloss.item())\n",
        "print(\"Critic Loss:\", critiloss.item())\n",
        "entropyb = entropies.mean()\n",
        "print(\"Entropy Bonus:\", entropyb.item())\n",
        "totalloss = actoloss + critiloss - 0.01 * entropyb\n",
        "print(\"Total Loss:\", totalloss.item())\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name}: {param.data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "np40-qP7cQPi",
        "outputId": "83b708d9-8f76-4410-d106-22d58c3203b0"
      },
      "id": "np40-qP7cQPi",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actor Loss: 0.15495535731315613\n",
            "Critic Loss: 0.8752533793449402\n",
            "Entropy Bonus: 0.6888728141784668\n",
            "Total Loss: 1.023319959640503\n",
            "shared_base.0.weight: tensor([[ 0.0361,  0.2982, -0.4571, -0.4232],\n",
            "        [-0.2571,  0.3197,  0.3343, -0.3405],\n",
            "        [ 0.0943, -0.2639, -0.2331,  0.4322],\n",
            "        [ 0.2405, -0.1357,  0.0726,  0.0959],\n",
            "        [ 0.1161, -0.1701, -0.2954,  0.3320],\n",
            "        [ 0.0662,  0.2103, -0.1904,  0.2412],\n",
            "        [ 0.3629,  0.3051,  0.1349,  0.4909],\n",
            "        [-0.0011,  0.1064,  0.4751,  0.4859],\n",
            "        [-0.2121,  0.4514,  0.1835, -0.0550],\n",
            "        [ 0.4376, -0.2600,  0.3259,  0.4965],\n",
            "        [-0.3439, -0.2960, -0.4945, -0.0910],\n",
            "        [-0.4531, -0.1665,  0.1528,  0.3524],\n",
            "        [ 0.2795,  0.2765,  0.2084, -0.3091],\n",
            "        [-0.3970,  0.0207,  0.4748,  0.0282],\n",
            "        [-0.0430, -0.3394, -0.2479, -0.3854],\n",
            "        [ 0.4013, -0.0825, -0.4407,  0.0546],\n",
            "        [ 0.0353, -0.0608,  0.1780,  0.2699],\n",
            "        [-0.4692, -0.3820, -0.0230, -0.4646],\n",
            "        [-0.4552,  0.4405, -0.0370, -0.2482],\n",
            "        [-0.1705, -0.3894,  0.3912, -0.1094],\n",
            "        [-0.2821, -0.3802,  0.2912, -0.0681],\n",
            "        [-0.0525, -0.1303, -0.2414,  0.0889],\n",
            "        [ 0.3837, -0.2888,  0.0230, -0.3813],\n",
            "        [-0.2334,  0.4379,  0.0062,  0.4807],\n",
            "        [-0.0866, -0.2735, -0.2977,  0.4422],\n",
            "        [-0.3835,  0.2614,  0.1253,  0.1466],\n",
            "        [ 0.2811, -0.1298,  0.3084, -0.3994],\n",
            "        [-0.1759,  0.3660,  0.1484, -0.0666],\n",
            "        [-0.1168, -0.3053,  0.3040, -0.0159],\n",
            "        [-0.0179,  0.1172,  0.3680, -0.2541],\n",
            "        [-0.1712,  0.3378,  0.2834, -0.2470],\n",
            "        [-0.4936,  0.0938,  0.2619,  0.1204],\n",
            "        [ 0.3537, -0.3957,  0.2511, -0.3098],\n",
            "        [ 0.2824,  0.3709,  0.1594,  0.4055],\n",
            "        [-0.3286, -0.4482, -0.4798,  0.3129],\n",
            "        [ 0.1742, -0.3651,  0.2411,  0.4552],\n",
            "        [-0.4940,  0.3637,  0.0757,  0.0475],\n",
            "        [ 0.4489,  0.4538,  0.2097,  0.3961],\n",
            "        [ 0.2090,  0.0805, -0.4385,  0.0924],\n",
            "        [-0.3567,  0.1549,  0.0872,  0.0122],\n",
            "        [-0.3409, -0.3041,  0.3706,  0.2445],\n",
            "        [ 0.2646, -0.1500, -0.0871, -0.1271],\n",
            "        [ 0.3428, -0.2148,  0.3139,  0.3437],\n",
            "        [-0.4563,  0.0041, -0.0942, -0.2774],\n",
            "        [ 0.2149, -0.0617,  0.4368, -0.3120],\n",
            "        [ 0.2323, -0.4269, -0.2506,  0.0341],\n",
            "        [ 0.3940,  0.2644, -0.4115,  0.0564],\n",
            "        [-0.2937,  0.0227, -0.4175, -0.1457],\n",
            "        [ 0.3419, -0.4578, -0.4777,  0.0901],\n",
            "        [ 0.0695, -0.0904,  0.4376, -0.3020],\n",
            "        [-0.3035, -0.3737, -0.0931, -0.1276],\n",
            "        [ 0.3084, -0.0271,  0.1587, -0.2088],\n",
            "        [ 0.4046,  0.4225, -0.4488, -0.1275],\n",
            "        [-0.4232,  0.2114, -0.4358, -0.2374],\n",
            "        [ 0.4986, -0.4765,  0.3679,  0.0412],\n",
            "        [ 0.3988, -0.2527,  0.3274,  0.0593],\n",
            "        [ 0.0375,  0.3423, -0.4763, -0.4017],\n",
            "        [ 0.4819, -0.2992, -0.3614, -0.0234],\n",
            "        [-0.1611, -0.0674, -0.3360,  0.1814],\n",
            "        [-0.4292,  0.0415,  0.2908,  0.0971],\n",
            "        [ 0.0664,  0.0731, -0.1465, -0.0952],\n",
            "        [-0.2118, -0.3368, -0.2657,  0.4953],\n",
            "        [-0.0847,  0.1741,  0.3448, -0.0401],\n",
            "        [-0.2700, -0.3378, -0.1852,  0.1795],\n",
            "        [-0.4221, -0.0600,  0.1892,  0.4089],\n",
            "        [ 0.1495,  0.0999,  0.3413,  0.2300],\n",
            "        [ 0.1692, -0.1550, -0.4009, -0.1080],\n",
            "        [-0.0907,  0.0367, -0.0550,  0.2021],\n",
            "        [ 0.3313, -0.2017, -0.4754, -0.2575],\n",
            "        [-0.2382,  0.2824, -0.3215, -0.0480],\n",
            "        [-0.1070, -0.3045,  0.2256, -0.2073],\n",
            "        [-0.0259, -0.3515, -0.0504,  0.0710],\n",
            "        [-0.0193, -0.1878,  0.1495, -0.1285],\n",
            "        [-0.2772, -0.3687, -0.1059, -0.4723],\n",
            "        [ 0.4160, -0.2399, -0.2062,  0.0472],\n",
            "        [ 0.1286, -0.0481,  0.3420, -0.2905],\n",
            "        [ 0.1802, -0.0021, -0.4057, -0.1946],\n",
            "        [ 0.1282,  0.0945,  0.3646,  0.3335],\n",
            "        [-0.2717, -0.2145, -0.1050, -0.3592],\n",
            "        [-0.1360, -0.4082,  0.1912,  0.3528],\n",
            "        [ 0.0386, -0.3569, -0.3797, -0.1936],\n",
            "        [ 0.1444,  0.0132,  0.0899,  0.4722],\n",
            "        [ 0.2332,  0.1065, -0.1002,  0.2355],\n",
            "        [ 0.1829, -0.4501, -0.2954,  0.0168],\n",
            "        [ 0.1887,  0.2423, -0.2986, -0.4951],\n",
            "        [-0.0260, -0.0572,  0.1451, -0.1375],\n",
            "        [ 0.3813,  0.0344,  0.3641, -0.3472],\n",
            "        [ 0.2000,  0.0596, -0.2501,  0.2479],\n",
            "        [-0.4910,  0.4032,  0.0413,  0.0473],\n",
            "        [ 0.2302,  0.0425,  0.2133,  0.3171],\n",
            "        [-0.3892,  0.3318, -0.0739, -0.1173],\n",
            "        [ 0.0996,  0.3921, -0.2728, -0.3360],\n",
            "        [ 0.3113, -0.2358,  0.1620, -0.0747],\n",
            "        [ 0.1366,  0.3826, -0.2544, -0.2416],\n",
            "        [ 0.2443,  0.0264, -0.0560, -0.4382],\n",
            "        [ 0.4246, -0.0199, -0.0911, -0.2042],\n",
            "        [-0.0977,  0.2420, -0.0105,  0.4266],\n",
            "        [-0.0279, -0.0707,  0.2832, -0.0139],\n",
            "        [-0.0988,  0.1681,  0.3953,  0.1238],\n",
            "        [ 0.3492,  0.3230,  0.1424,  0.1748],\n",
            "        [ 0.1613,  0.0519,  0.4254,  0.4616],\n",
            "        [ 0.1790, -0.0260,  0.4493, -0.3902],\n",
            "        [ 0.3664,  0.2659,  0.3221, -0.0394],\n",
            "        [-0.3720,  0.3557, -0.2176,  0.1306],\n",
            "        [ 0.2843, -0.4683,  0.0342, -0.1766],\n",
            "        [ 0.3539, -0.3174,  0.3026,  0.3128],\n",
            "        [ 0.2749,  0.4810,  0.0767, -0.4542],\n",
            "        [ 0.3559,  0.0468,  0.0089, -0.4753],\n",
            "        [-0.1114, -0.2437,  0.4907,  0.3570],\n",
            "        [ 0.3720,  0.3434, -0.3109, -0.2137],\n",
            "        [-0.1957, -0.2004,  0.3892,  0.1699],\n",
            "        [ 0.3997, -0.0224, -0.4467, -0.1461],\n",
            "        [ 0.3524,  0.3034,  0.1558, -0.1477],\n",
            "        [-0.4657, -0.2677,  0.2617, -0.2027],\n",
            "        [ 0.2382,  0.1375, -0.1930,  0.3055],\n",
            "        [-0.1617, -0.0176, -0.1895, -0.4753],\n",
            "        [ 0.0816,  0.1435, -0.2507,  0.3057],\n",
            "        [-0.4475, -0.4810, -0.3098, -0.1718],\n",
            "        [-0.0545, -0.0956,  0.2292, -0.4398],\n",
            "        [ 0.0746,  0.0077,  0.1331,  0.4960],\n",
            "        [ 0.3087, -0.2779, -0.4087, -0.1221],\n",
            "        [-0.2518,  0.1757,  0.0036,  0.4032],\n",
            "        [ 0.1974, -0.0929,  0.0182,  0.1207],\n",
            "        [-0.0630,  0.4045, -0.3920,  0.2447],\n",
            "        [-0.3197, -0.4370, -0.1036,  0.2447],\n",
            "        [ 0.3543,  0.1245, -0.0422, -0.0290],\n",
            "        [-0.2017, -0.0459, -0.0335,  0.4245],\n",
            "        [ 0.2936, -0.4011,  0.0031, -0.3085]])\n",
            "shared_base.0.bias: tensor([-8.0370e-02, -3.9838e-01,  1.2898e-01, -3.6154e-01,  4.9261e-01,\n",
            "         3.7860e-01, -3.4518e-01, -4.7073e-01,  1.8509e-02, -1.3858e-01,\n",
            "         1.4578e-01, -3.3946e-01,  2.0567e-01, -4.3187e-01, -2.6389e-01,\n",
            "        -2.6185e-01,  2.4095e-01, -1.4091e-02, -4.8151e-01, -2.8456e-01,\n",
            "        -2.3039e-01, -4.6391e-01,  6.3023e-02,  1.7917e-01, -9.0739e-02,\n",
            "        -1.9997e-01, -4.0323e-03, -3.9714e-01, -3.0445e-01, -1.2403e-02,\n",
            "         4.4519e-01, -1.5275e-01,  2.4683e-01, -1.9578e-01, -1.3410e-01,\n",
            "        -4.7697e-01,  4.1524e-01,  4.8586e-01, -1.1502e-02,  3.1868e-01,\n",
            "         4.0997e-01,  2.4736e-01,  2.5651e-01, -4.7700e-01, -4.7084e-01,\n",
            "        -3.8377e-01,  1.5539e-01, -4.3149e-01, -3.8267e-01, -2.4135e-01,\n",
            "        -4.6084e-01, -3.2629e-04,  7.1134e-02, -2.9360e-01,  3.2638e-01,\n",
            "        -1.9082e-01, -1.3976e-01,  3.8194e-01,  4.2828e-01, -5.6360e-02,\n",
            "        -3.5894e-01, -2.4983e-01,  6.4670e-02,  2.9210e-01,  2.4992e-01,\n",
            "        -2.2179e-01, -4.7268e-01, -4.3757e-01, -1.6008e-01,  5.2721e-03,\n",
            "        -2.2559e-01, -3.9535e-01,  5.3694e-02, -3.9687e-01, -3.8296e-01,\n",
            "        -4.0747e-01, -1.3649e-01,  4.4263e-01,  1.9646e-01, -6.0285e-02,\n",
            "         1.0191e-01,  1.9810e-01,  1.9131e-03, -4.0329e-01, -3.0906e-01,\n",
            "        -3.3968e-01, -1.3696e-02, -4.5770e-01, -9.1911e-02, -1.8384e-01,\n",
            "         4.9169e-01,  4.0541e-01,  2.9143e-01,  2.7041e-02, -1.6289e-01,\n",
            "         1.2854e-01, -3.4646e-01, -2.1225e-01, -9.9443e-02, -1.1648e-01,\n",
            "        -4.7365e-01,  1.9977e-01,  4.1274e-01, -2.7679e-01,  1.3578e-01,\n",
            "        -5.8987e-02, -1.9236e-02,  2.8145e-02,  4.3429e-01,  6.5995e-02,\n",
            "        -3.1634e-01, -1.8086e-01,  2.4526e-01,  1.5703e-01,  1.0718e-02,\n",
            "        -6.6988e-02, -1.9554e-01, -1.7646e-01, -2.6274e-01,  8.2931e-02,\n",
            "         1.4142e-01,  3.8187e-01, -1.1196e-01,  3.6794e-02,  3.2214e-01,\n",
            "         2.1850e-01,  6.5244e-02,  1.5464e-01])\n",
            "actor_head.weight: tensor([[ 5.5879e-03, -7.4819e-02,  2.4075e-03, -2.0320e-02,  7.8763e-02,\n",
            "          8.0782e-02,  1.6535e-02,  7.9002e-02, -4.4522e-02,  7.5825e-02,\n",
            "          7.7101e-02, -4.3783e-03, -7.1932e-02, -2.8523e-02, -2.3637e-02,\n",
            "          4.6000e-04, -8.1137e-02,  1.5381e-02, -3.5760e-02, -2.1631e-02,\n",
            "         -4.6652e-02, -1.8662e-02, -8.3088e-02, -4.5861e-02,  9.1428e-03,\n",
            "          5.8919e-02,  8.1323e-02, -6.6862e-02,  3.7116e-02, -4.8824e-02,\n",
            "         -6.7338e-02, -4.8922e-03, -5.1704e-02, -7.7124e-02, -8.1602e-02,\n",
            "         -6.0163e-02,  7.3632e-02,  4.8295e-02, -3.3988e-02, -5.8160e-02,\n",
            "         -6.8992e-02, -5.3195e-02, -3.7372e-02,  5.2085e-02,  2.9926e-02,\n",
            "         -3.4062e-02,  6.1849e-02, -5.3910e-02, -7.3009e-02,  2.0291e-02,\n",
            "         -4.8737e-02,  2.4145e-02,  7.5886e-02, -4.0799e-02, -6.2678e-02,\n",
            "          3.2269e-02, -7.8828e-02,  7.7858e-02, -4.0742e-02,  4.1732e-02,\n",
            "         -7.0284e-02, -5.8827e-02,  1.0736e-02,  3.2414e-02,  6.9536e-02,\n",
            "          7.2922e-02, -5.0834e-03, -1.1364e-02,  1.3611e-02, -1.9532e-02,\n",
            "          4.0088e-03, -8.3708e-02, -7.1559e-02,  6.0569e-02, -5.2650e-02,\n",
            "          7.0605e-02,  1.0700e-02,  2.0530e-02,  8.1334e-02, -5.0161e-02,\n",
            "         -1.2953e-02,  1.4660e-02,  2.9197e-02, -8.6021e-02,  3.2415e-02,\n",
            "         -5.2125e-02,  1.0610e-02,  5.9799e-02, -7.8048e-03, -3.7961e-03,\n",
            "         -5.6040e-03,  7.4759e-02,  4.9234e-02,  1.5864e-02, -5.2193e-02,\n",
            "         -4.8998e-02,  7.1406e-02,  6.4587e-02,  5.2178e-02, -6.9918e-02,\n",
            "          6.8836e-02, -7.4492e-03,  6.4121e-02,  7.7770e-02,  8.6883e-03,\n",
            "         -3.7780e-02,  8.4143e-02, -4.5061e-02,  5.6754e-02, -8.3755e-02,\n",
            "         -2.7799e-02, -7.8062e-02, -4.0649e-02,  3.6531e-02, -6.4333e-02,\n",
            "         -7.5439e-02,  6.4201e-02,  4.6184e-02,  3.3456e-05, -6.3310e-02,\n",
            "         -2.4417e-02, -2.7618e-02,  5.3588e-02, -2.7911e-02, -2.4457e-02,\n",
            "         -6.3655e-02, -4.0630e-02,  4.6858e-02],\n",
            "        [ 5.5947e-02,  1.1842e-02, -3.2163e-02,  8.5896e-02,  1.8446e-02,\n",
            "         -3.5118e-02, -2.9635e-02, -1.2370e-02,  4.0905e-02,  3.3330e-02,\n",
            "          6.3078e-02, -2.9825e-03,  7.4105e-02, -3.5902e-02, -4.0708e-02,\n",
            "         -3.1642e-02, -8.1927e-02, -5.7022e-02, -8.0980e-02, -6.2235e-03,\n",
            "          1.8333e-03, -2.2940e-02, -5.9403e-02,  7.9194e-02, -3.0287e-02,\n",
            "          4.1122e-02,  6.0555e-02, -5.8103e-02, -1.1051e-02,  5.0086e-02,\n",
            "          8.4680e-02, -1.0151e-02,  6.5048e-02, -6.8373e-02, -5.8429e-02,\n",
            "          7.3685e-02, -9.3686e-03,  8.8321e-02,  5.4274e-03, -3.7655e-02,\n",
            "          8.4944e-02,  7.8294e-02,  9.2030e-03, -5.2581e-02, -4.8269e-03,\n",
            "         -4.2507e-02,  6.3407e-02,  3.0945e-03,  1.9608e-02,  5.2604e-02,\n",
            "          2.9278e-02, -5.4178e-02,  7.2426e-02,  5.3301e-02, -5.3565e-02,\n",
            "          6.7831e-02,  2.4076e-02, -1.6399e-02, -6.4212e-02, -1.3666e-02,\n",
            "         -8.6660e-02,  4.9449e-02,  6.3016e-02, -2.6447e-02,  8.0940e-02,\n",
            "         -3.1729e-02,  1.5136e-02, -6.1597e-02, -2.7369e-03,  3.4729e-02,\n",
            "          3.7072e-02, -2.9639e-03, -4.5571e-02,  6.1782e-02,  5.9519e-02,\n",
            "          1.3185e-02, -4.1648e-02,  1.9741e-02, -7.6365e-04,  8.1752e-02,\n",
            "          1.2099e-02,  7.8173e-03,  7.0422e-03,  4.7023e-02, -2.0921e-03,\n",
            "          7.5762e-02,  4.7049e-02, -6.6789e-02, -1.0010e-03, -7.4025e-02,\n",
            "         -6.8865e-02,  1.6173e-02,  2.0863e-02, -8.7351e-03,  8.3010e-02,\n",
            "         -4.6483e-02, -3.8042e-02, -1.5154e-02, -4.6408e-04, -6.4867e-03,\n",
            "          7.9584e-02, -3.8330e-02, -4.5234e-02,  6.5970e-03,  6.7387e-02,\n",
            "          2.1979e-02,  4.6618e-02,  1.6111e-02,  7.6332e-02,  5.2088e-02,\n",
            "          6.3248e-02, -1.0905e-02, -7.2176e-02, -4.0928e-02,  3.1111e-02,\n",
            "          6.1009e-02, -3.5534e-02,  7.4134e-02,  1.1704e-02,  1.3511e-03,\n",
            "          4.4458e-03, -1.6616e-02,  8.3748e-02,  2.8391e-03,  5.3139e-02,\n",
            "         -7.3601e-02, -4.6540e-02,  1.0938e-02]])\n",
            "actor_head.bias: tensor([0.0696, 0.0431])\n",
            "critic_head.weight: tensor([[ 0.0158, -0.0863,  0.0027, -0.0573, -0.0745,  0.0111, -0.0549,  0.0802,\n",
            "         -0.0490,  0.0295,  0.0509, -0.0677,  0.0462, -0.0118,  0.0458,  0.0865,\n",
            "         -0.0367, -0.0373,  0.0692,  0.0108, -0.0078,  0.0671, -0.0417, -0.0054,\n",
            "          0.0597,  0.0335, -0.0562,  0.0230, -0.0674, -0.0470,  0.0578, -0.0226,\n",
            "         -0.0440,  0.0073,  0.0206, -0.0539, -0.0606, -0.0846,  0.0218, -0.0580,\n",
            "          0.0077, -0.0188, -0.0568, -0.0272, -0.0669, -0.0468, -0.0038, -0.0766,\n",
            "         -0.0765,  0.0672,  0.0083, -0.0381, -0.0039,  0.0645, -0.0727, -0.0459,\n",
            "         -0.0649, -0.0054, -0.0040, -0.0679,  0.0613, -0.0689,  0.0835, -0.0618,\n",
            "         -0.0131,  0.0175, -0.0393,  0.0813, -0.0599, -0.0059, -0.0639,  0.0245,\n",
            "          0.0764,  0.0820,  0.0115, -0.0433,  0.0849,  0.0076,  0.0608, -0.0449,\n",
            "         -0.0420, -0.0724,  0.0823,  0.0147, -0.0625,  0.0076, -0.0369,  0.0044,\n",
            "          0.0283, -0.0429,  0.0144, -0.0568,  0.0180, -0.0863,  0.0151,  0.0888,\n",
            "          0.0804,  0.0332,  0.0660,  0.0869,  0.0250,  0.0305,  0.0125,  0.0773,\n",
            "          0.0769, -0.0406, -0.0026, -0.0435,  0.0327, -0.0846,  0.0313, -0.0233,\n",
            "          0.0331,  0.0286, -0.0430,  0.0469,  0.0365, -0.0741,  0.0892,  0.0771,\n",
            "          0.0560, -0.0100,  0.0323, -0.0488, -0.0310,  0.0427,  0.0383,  0.0360]])\n",
            "critic_head.bias: tensor([-0.0833])\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "a974e302d1fdb028"
      },
      "cell_type": "markdown",
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:"
      ],
      "id": "a974e302d1fdb028"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The shared actor-critic architecture gives a highly efficient model. Instead of using two separate networks, the policy (actor) and value (critic) share the same feature extractor which means there are fewer parameters and this helps the model to learn faster.\n",
        "Computation needs to be efficient. Actor and critic benefit in environments where there is value estimation and policy decisions rely on the same aspects of the input. The main aim is to reduce overfitting which is a form of regularization.\n",
        "But, in more complex environments, separate networks may perform better.\n",
        "\n"
      ],
      "metadata": {
        "id": "v5Te2-pWfAWz"
      },
      "id": "v5Te2-pWfAWz"
    },
    {
      "metadata": {
        "id": "eb645eb009b85b1c"
      },
      "cell_type": "markdown",
      "source": [
        "## Section 2: Auto-Adaptive Network Setup for Environments\n",
        "\n",
        "You will now create a function that builds a shared actor-critic network that adapts to any Gymnasium environment. This function should inspect the environment and build input/output layers accordingly."
      ],
      "id": "eb645eb009b85b1c"
    },
    {
      "metadata": {
        "id": "4223b6ddf43abee5"
      },
      "cell_type": "markdown",
      "source": [
        "### Task 2: Auto-generate Input and Output Layers\n",
        "Write a function `create_shared_network(env)` that constructs a neural network using the following rules:\n",
        "- The input layer should match the environment's observation space.\n",
        "- The output layer for the **actor** should depend on the action space:\n",
        "  - For discrete actions: output probabilities using `nn.Softmax`.\n",
        "  - For continuous actions: output mean and log std for a Gaussian distribution.\n",
        "- The **critic** always outputs a single scalar value.\n",
        "\n",
        "```python\n",
        "# TODO: Define function `create_shared_network(env)`\n",
        "```\n",
        "\n",
        "#### Environments to Support:\n",
        "Test your function with the following environments:\n",
        "1. `CliffWalking-v0` (Use one-hot encoding for discrete integer observations.)\n",
        "2. `LunarLander-v3` (Standard Box space for observations and discrete actions.)\n",
        "3. `PongNoFrameskip-v4` (Use gym wrappers for Atari image preprocessing.)\n",
        "4. `HalfCheetah-v5` (Continuous observation and continuous action.)\n",
        "\n",
        "```python\n",
        "# TODO: Loop through environments and test `create_shared_network`\n",
        "```\n",
        "\n",
        "Hint: Use `gym.spaces` utilities to determine observation/action types dynamically.\n",
        "\n",
        "ðŸ”— Observation/Action Space Docs:\n",
        "- https://gymnasium.farama.org/api/spaces/\n",
        "\n",
        "---"
      ],
      "id": "4223b6ddf43abee5"
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install ale-py\n",
        "%pip install autorom==0.6.1\n",
        "!AutoROM --accept-license\n",
        "%pip install \"gymnasium[atari,accept-rom-license]\"\n",
        "%pip show gymnasium\n",
        "%pip install \"gymnasium[mujoco]\"\n",
        "%pip install swig\n",
        "%pip install \"gymnasium[box2d]\"\n",
        "\n",
        "import gymnasium as gym\n",
        "import ale_py\n",
        "\n",
        "gym.register_envs(ale_py)\n",
        "%pip install \"gymnasium[other]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTFaellt0oE2",
        "outputId": "0d79a9d2-908d-43e5-bd84-7d8199f86fe4"
      },
      "id": "vTFaellt0oE2",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ale-py in /usr/local/lib/python3.11/dist-packages (0.10.2)\n",
            "Requirement already satisfied: numpy>1.20 in /usr/local/lib/python3.11/dist-packages (from ale-py) (2.0.2)\n",
            "Collecting autorom==0.6.1\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from autorom==0.6.1) (8.1.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from autorom==0.6.1) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->autorom==0.6.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->autorom==0.6.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->autorom==0.6.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->autorom==0.6.1) (2025.1.31)\n",
            "Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Installing collected packages: autorom\n",
            "Successfully installed autorom-0.6.1\n",
            "AutoROM will download the Atari 2600 ROMs.\n",
            "They will be installed to:\n",
            "\t/usr/local/lib/python3.11/dist-packages/AutoROM/roms\n",
            "\n",
            "Existing ROMs will be overwritten.\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/adventure.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/air_raid.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/alien.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/amidar.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/assault.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/asterix.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/asteroids.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/atlantis.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/atlantis2.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/backgammon.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/bank_heist.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/basic_math.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/battle_zone.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/beam_rider.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/berzerk.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/blackjack.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/bowling.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/boxing.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/breakout.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/carnival.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/casino.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/centipede.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/chopper_command.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/combat.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/crazy_climber.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/crossbow.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/darkchambers.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/defender.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/demon_attack.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/donkey_kong.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/double_dunk.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/earthworld.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/elevator_action.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/enduro.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/entombed.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/et.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/fishing_derby.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/flag_capture.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/freeway.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/frogger.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/frostbite.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/galaxian.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/gopher.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/gravitar.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/hangman.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/haunted_house.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/hero.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/human_cannonball.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/ice_hockey.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/jamesbond.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/journey_escape.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/joust.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/kaboom.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/kangaroo.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/keystone_kapers.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/king_kong.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/klax.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/koolaid.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/krull.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/kung_fu_master.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/laser_gates.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/lost_luggage.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/mario_bros.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/maze_craze.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/miniature_golf.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/montezuma_revenge.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/mr_do.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/ms_pacman.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/name_this_game.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/othello.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/pacman.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/phoenix.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/pitfall.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/pitfall2.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/pong.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/pooyan.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/private_eye.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/qbert.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/riverraid.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/road_runner.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/robotank.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/seaquest.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/sir_lancelot.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/skiing.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/solaris.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/space_invaders.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/space_war.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/star_gunner.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/superman.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/surround.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/tennis.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/tetris.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/tic_tac_toe_3d.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/time_pilot.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/trondead.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/turmoil.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/tutankham.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/up_n_down.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/venture.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/video_checkers.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/video_chess.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/video_cube.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/video_pinball.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/warlords.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/wizard_of_wor.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/word_zapper.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/yars_revenge.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/zaxxon.bin\n",
            "Done!\n",
            "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "\u001b[33mWARNING: gymnasium 1.1.1 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license,atari]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license,atari]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license,atari]) (4.13.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
            "Requirement already satisfied: ale_py>=0.9 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license,atari]) (0.10.2)\n",
            "Name: gymnasium\n",
            "Version: 1.1.1\n",
            "Summary: A standard API for reinforcement learning and a diverse set of reference environments (formerly Gym).\n",
            "Home-page: https://farama.org\n",
            "Author: \n",
            "Author-email: Farama Foundation <contact@farama.org>\n",
            "License: MIT License\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: cloudpickle, farama-notifications, numpy, typing-extensions\n",
            "Required-by: dopamine_rl\n",
            "Requirement already satisfied: gymnasium[mujoco] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (4.13.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (0.0.4)\n",
            "Collecting mujoco>=2.1.5 (from gymnasium[mujoco])\n",
            "  Downloading mujoco-3.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (2.37.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio>=2.14.1->gymnasium[mujoco]) (11.1.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.12.2)\n",
            "Collecting glfw (from mujoco>=2.1.5->gymnasium[mujoco])\n",
            "  Downloading glfw-2.9.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (3.1.9)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (2025.3.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (3.21.0)\n",
            "Downloading mujoco-3.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading glfw-2.9.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl (243 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m243.5/243.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: glfw, mujoco\n",
            "Successfully installed glfw-2.9.0 mujoco-3.3.1\n",
            "Collecting swig\n",
            "  Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.3.0\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.13.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2379496 sha256=7156333ff1c8b1819c4c038d1e469a15adf7cf5ba15a416aa9411e92f34389bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.5\n",
            "Requirement already satisfied: gymnasium[other] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[other]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[other]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[other]) (4.13.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[other]) (0.0.4)\n",
            "Requirement already satisfied: moviepy>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[other]) (1.0.3)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[other]) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[other]) (4.11.0.86)\n",
            "Requirement already satisfied: seaborn>=0.13 in /usr/local/lib/python3.11/dist-packages (from gymnasium[other]) (0.13.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->gymnasium[other]) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->gymnasium[other]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->gymnasium[other]) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->gymnasium[other]) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->gymnasium[other]) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->gymnasium[other]) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->gymnasium[other]) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->gymnasium[other]) (2.8.2)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.11/dist-packages (from moviepy>=1.0.0->gymnasium[other]) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.11/dist-packages (from moviepy>=1.0.0->gymnasium[other]) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from moviepy>=1.0.0->gymnasium[other]) (2.32.3)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.11/dist-packages (from moviepy>=1.0.0->gymnasium[other]) (0.1.11)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.11/dist-packages (from moviepy>=1.0.0->gymnasium[other]) (2.37.0)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from moviepy>=1.0.0->gymnasium[other]) (0.6.0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.13->gymnasium[other]) (2.2.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn>=0.13->gymnasium[other]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn>=0.13->gymnasium[other]) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->gymnasium[other]) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[other]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[other]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[other]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[other]) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "d6d249ff9277403a"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium.spaces import Box, Discrete\n",
        "from gymnasium.wrappers import AtariPreprocessing, ResizeObservation\n",
        "\n",
        "class SharedActorCritic(nn.Module):\n",
        "    def __init__(self, input_shape, action_space, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.action_space = action_space\n",
        "        self.use_cnn = len(input_shape) == 3\n",
        "\n",
        "        if self.use_cnn:\n",
        "            self.conv = nn.Sequential(\n",
        "                nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "                nn.ReLU(),\n",
        "                nn.Flatten()\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                dummy = torch.zeros(1, *input_shape)\n",
        "                conv_out_size = self.conv(dummy).shape[1]\n",
        "            self.shared = nn.Sequential(\n",
        "                nn.Linear(conv_out_size, hidden_dim),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "        else:\n",
        "            self.shared = nn.Sequential(\n",
        "                nn.Linear(np.prod(input_shape), hidden_dim),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "\n",
        "        if isinstance(action_space, Discrete):\n",
        "            self.actor = nn.Sequential(\n",
        "                nn.Linear(hidden_dim, action_space.n),\n",
        "                nn.Softmax(dim=-1)\n",
        "            )\n",
        "        else:\n",
        "            act_dim = action_space.shape[0]\n",
        "            self.actor_mean = nn.Linear(hidden_dim, act_dim)\n",
        "            self.actor_log_std = nn.Parameter(torch.zeros(act_dim))\n",
        "        self.critic = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_cnn:\n",
        "            x = x / 255.0\n",
        "            x = self.conv(x)\n",
        "        else:\n",
        "            x = x.view(x.size(0), -1)\n",
        "        x = self.shared(x)\n",
        "\n",
        "        if isinstance(self.action_space, Discrete):\n",
        "            return self.actor(x), self.critic(x)\n",
        "        else:\n",
        "            mean = self.actor_mean(x)\n",
        "            std = self.actor_log_std.exp().expand_as(mean)\n",
        "            return (mean, std), self.critic(x)\n"
      ],
      "id": "d6d249ff9277403a",
      "outputs": [],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": [
        "def create_shared_network(env):\n",
        "    obs_space = env.observation_space\n",
        "    act_space = env.action_space\n",
        "\n",
        "    if isinstance(obs_space, Discrete):\n",
        "        input_shape = (obs_space.n,)\n",
        "        one_hot = True\n",
        "    elif isinstance(obs_space, Box):\n",
        "        input_shape = obs_space.shape\n",
        "        one_hot = False\n",
        "    else:\n",
        "        raise NotImplementedError(\"notsupported\")\n",
        "\n",
        "    model = SharedActorCritic(input_shape, act_space)\n",
        "    return model, one_hot\n"
      ],
      "metadata": {
        "id": "4000p11yYrsv"
      },
      "id": "4000p11yYrsv",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_obs(obs, one_hot, obs_space):\n",
        "    if one_hot:\n",
        "        vec = np.zeros(obs_space.n, dtype=np.float32)\n",
        "        vec[obs] = 1.0\n",
        "        return torch.tensor(vec).unsqueeze(0)\n",
        "    elif len(obs.shape) == 2:\n",
        "        obs = np.expand_dims(obs, axis=0)\n",
        "        return torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
        "    elif len(obs.shape) == 3:\n",
        "        obs = np.transpose(obs, (2, 0, 1))\n",
        "        return torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
        "    else:\n",
        "        return torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n"
      ],
      "metadata": {
        "id": "ZzT7JVv6YtgA"
      },
      "id": "ZzT7JVv6YtgA",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_ids = [\n",
        "    \"CliffWalking-v0\",\n",
        "    \"LunarLander-v3\",\n",
        "    \"PongNoFrameskip-v4\",\n",
        "    \"HalfCheetah-v5\"\n",
        "]\n",
        "\n",
        "for env_id in env_ids:\n",
        "    print(f\"\\nTesting {env_id}\")\n",
        "\n",
        "    if \"Pong\" in env_id:\n",
        "        env = gym.make(env_id)\n",
        "        env = AtariPreprocessing(env, grayscale_obs=True)\n",
        "        env = ResizeObservation(env, shape=(84, 84))\n",
        "    else:\n",
        "        env = gym.make(env_id)\n",
        "\n",
        "    model, one_hot = create_shared_network(env)\n",
        "    obs, _ = env.reset()\n",
        "    input_tensor = preprocess_obs(obs, one_hot, env.observation_space)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "\n",
        "    if isinstance(env.action_space, Discrete):\n",
        "        print(\"Action probabilities:\", output[0])\n",
        "    else:\n",
        "        print(\" Action mean/std:\", output[0])\n",
        "\n",
        "    print(\"Value estimate:\", output[1])\n",
        "    env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spvl8X35Yvc3",
        "outputId": "286221d1-b296-4716-f7cc-b1c5d062d8e3"
      },
      "id": "spvl8X35Yvc3",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing CliffWalking-v0\n",
            "Action probabilities: tensor([[0.2409, 0.2486, 0.2339, 0.2766]])\n",
            "Value estimate: tensor([[-0.0473]])\n",
            "\n",
            "Testing LunarLander-v3\n",
            "Action probabilities: tensor([[0.2224, 0.2819, 0.2542, 0.2415]])\n",
            "Value estimate: tensor([[0.0667]])\n",
            "\n",
            "Testing PongNoFrameskip-v4\n",
            "Action probabilities: tensor([[1.3875e-36, 1.0000e+00, 2.1152e-40, 3.0886e-19, 2.4227e-20, 1.5432e-35]])\n",
            "Value estimate: tensor([[3.8886]])\n",
            "\n",
            "Testing HalfCheetah-v5\n",
            " Action mean/std: (tensor([[ 0.0579, -0.0120,  0.1169, -0.0969, -0.0237, -0.0091]]), tensor([[1., 1., 1., 1., 1., 1.]]))\n",
            "Value estimate: tensor([[-0.0988]])\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "4ccd13f0b62b30ff"
      },
      "cell_type": "markdown",
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:"
      ],
      "id": "4ccd13f0b62b30ff"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the observations and action spaces:\n",
        "CliffWalking-v0: One-hot encoding it avoids the false ordering in discrete states making sure that there is clear state separation.\n",
        "\n",
        "\n",
        "LunarLander-v3: It is the standard Box observations  which are fed directly into the network and they  are suitable for low-dimensional continuous input.\n",
        "\n",
        "Pong-v4: It uses image-based inputs, preprocessing which reduces  the complexity even with preserving the structure.\n",
        "\n",
        "HalfCheetah-v5: The actions require outputting mean and log std for a Gaussian policy which are ideal for smooth control.\n",
        "The shared base with separate actor and critic heads improves efficiency and feature learning.\n",
        "\n"
      ],
      "metadata": {
        "id": "FKY83CjrfPxt"
      },
      "id": "FKY83CjrfPxt"
    },
    {
      "metadata": {
        "id": "ee2dd81024ce246a"
      },
      "cell_type": "code",
      "source": [],
      "id": "ee2dd81024ce246a",
      "outputs": [],
      "execution_count": 16
    },
    {
      "metadata": {
        "id": "b39c886fa536a639"
      },
      "cell_type": "markdown",
      "source": [
        "### Task 3: Write Observation Normalization Function\n",
        "Create a function `normalize_observation(obs, env)` that:\n",
        "- Checks if the observation space is `Box` and has `low` and `high` attributes.\n",
        "- If so, normalize the input observation.\n",
        "- Otherwise, return the observation unchanged.\n",
        "\n",
        "```python\n",
        "# TODO: Define `normalize_observation(obs, env)`\n",
        "```\n",
        "\n",
        "Test this function with observations from:\n",
        "- `LunarLander-v3`\n",
        "- `PongNoFrameskip-v4`\n",
        "\n",
        "Note: Atari observations are image arrays. Normalize pixel values to [0, 1]. For LunarLander-v3, the different elements in the observation vector have different ranges. Normalize them to [0, 1] using the `low` and `high` attributes of the observation space.\n",
        "\n",
        "\n",
        "---"
      ],
      "id": "b39c886fa536a639"
    },
    {
      "metadata": {
        "id": "fc7ee06112cf7d29"
      },
      "cell_type": "code",
      "source": [
        "# BEGIN_YOUR_CODE\n",
        "import numpy as np\n",
        "\n",
        "def normalize_observation(obs, env):\n",
        "    obs_space = env.observation_space\n",
        "\n",
        "    if isinstance(obs_space, gym.spaces.Box):\n",
        "        if obs_space.dtype == np.uint8 or np.max(obs_space.high) > 1.0:\n",
        "\n",
        "            return obs.astype(np.float32) / 255.0 if obs.dtype == np.uint8 else \\\n",
        "                   (obs - obs_space.low) / (obs_space.high - obs_space.low + 1e-8)\n",
        "        else:\n",
        "\n",
        "            return obs\n",
        "    else:\n",
        "\n",
        "        return obs\n",
        "\n",
        "\n",
        "# END_YOUR_CODE"
      ],
      "id": "fc7ee06112cf7d29",
      "outputs": [],
      "execution_count": 17
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import AtariPreprocessing, ResizeObservation\n",
        "\n",
        "\n",
        "env1 = gym.make(\"LunarLander-v3\")\n",
        "obs1, _ = env1.reset()\n",
        "norm_obs1 = normalize_observation(obs1, env1)\n",
        "\n",
        "print(\"\\nLunarLander-v3\")\n",
        "print(\"Raw observation:\", obs1)\n",
        "print(\"Normalized:\", norm_obs1)\n",
        "\n",
        "\n",
        "env2 = gym.make(\"PongNoFrameskip-v4\")\n",
        "env2 = AtariPreprocessing(env2, grayscale_obs=True)\n",
        "env2 = ResizeObservation(env2, shape=(84, 84))\n",
        "obs2, _ = env2.reset()\n",
        "norm_obs2 = normalize_observation(obs2, env2)\n",
        "\n",
        "print(\"\\nPongNoFrameskip-v4\")\n",
        "print(\"Raw observation shape:\", obs2.shape, \"dtype:\", obs2.dtype)\n",
        "print(\"Normalized shape:\", norm_obs2.shape, \"min:\", np.min(norm_obs2), \"max:\", np.max(norm_obs2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "li4KG2ZiaeFq",
        "outputId": "9562b3d3-aad7-4253-f3f2-c5720b809721"
      },
      "id": "li4KG2ZiaeFq",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LunarLander-v3\n",
            "Raw observation: [ 4.2390823e-04  1.4205443e+00  4.2914189e-02  4.2773551e-01\n",
            " -4.8433049e-04 -9.7207259e-03  0.0000000e+00  0.0000000e+00]\n",
            "Normalized: [0.50008476 0.7841088  0.5021457  0.52138674 0.49996144 0.49951395\n",
            " 0.         0.        ]\n",
            "\n",
            "PongNoFrameskip-v4\n",
            "Raw observation shape: (84, 84) dtype: uint8\n",
            "Normalized shape: (84, 84) min: 0.20392157 max: 0.9254902\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "501ed2a6e7ca7a7b"
      },
      "cell_type": "markdown",
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:"
      ],
      "id": "501ed2a6e7ca7a7b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization is a done to ensure stable and efficient training. Different environments have different normalization:\n",
        "\n",
        "LunarLander-v3 gives a vector of continuous values, each with different scales. Without  the normalization the large-magnitude features can dominate learning. With the environmentâ€™s low and high bounds to scale values into [0, 1] make sure all features contribute equally to learning.\n",
        "\n",
        "PongNoFrameskip-v4 gives image-based observations as unit pixel arrays which range from 0 to 255.\n",
        "This setup is used because it prevents issues as uneven weight updates caused by scale differences, and allows the model to converge more smoothly.\n"
      ],
      "metadata": {
        "id": "DxoYD84Whc8Z"
      },
      "id": "DxoYD84Whc8Z"
    },
    {
      "metadata": {
        "id": "6b5fb5353307f514"
      },
      "cell_type": "markdown",
      "source": [
        "## Section 4: Gradient Clipping\n",
        "\n",
        "To prevent exploding gradients, it's common practice to clip gradients before optimizer updates.\n",
        "\n",
        "### Task 4: Clip Gradients for Actor-Critic Networks\n",
        "Use dummy tensors and apply gradient clipping with the following PyTorch method:\n",
        "```python\n",
        "# During training, after loss.backward():\n",
        "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "```\n",
        "\n",
        "Reuse the loss computation from Task 1a or 1b. After computing the gradients, apply gradient clipping.\n",
        "Print the gradient norm before and after clipping to verify itâ€™s applied.\n",
        "\n",
        "ðŸ”— PyTorch Docs: https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html\n",
        "\n",
        "\n",
        "---"
      ],
      "id": "6b5fb5353307f514"
    },
    {
      "metadata": {
        "id": "7327507fb6e803ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74bd3c31-498c-42ad-ccc8-ad5247cfcd9b"
      },
      "cell_type": "code",
      "source": [
        "# BEGIN_YOUR_CODE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class SharedActorCritic(nn.Module):\n",
        "    def __init__(self, stdim, actdim, hiddensiz=128):\n",
        "        super().__init__()\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(stdim, hiddensiz),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(hiddensiz, actdim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "        self.critic = nn.Linear(hiddensiz, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.shared(x)\n",
        "        return self.actor(x), self.critic(x)\n",
        "stdim = 4\n",
        "actdim = 2\n",
        "batchsiz = 5\n",
        "model = SharedActorCritic(stdim, actdim)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "states = torch.randn(batchsiz, stdim)\n",
        "actions = torch.randint(0, actdim, (batchsiz,))\n",
        "returns = torch.randn(batchsiz, 1)\n",
        "actn_p, values = model(states)\n",
        "values = values.squeeze(-1)\n",
        "dist = torch.distributions.Categorical(actn_p)\n",
        "logp = dist.log_prob(actions)\n",
        "entropies = dist.entropy()\n",
        "advantages = returns.squeeze() - values.detach()\n",
        "actoloss = -(logp * advantages).mean()\n",
        "critiloss = F.mse_loss(values, returns.squeeze())\n",
        "entropyb = entropies.mean()\n",
        "totalloss = actoloss + critiloss - 0.01 * entropyb\n",
        "optimizer.zero_grad()\n",
        "totalloss.backward()\n",
        "total_norm_before = torch.norm(\n",
        "    torch.stack([p.grad.norm(2) for p in model.parameters() if p.grad is not None])\n",
        ").item()\n",
        "print(f\"Gradient norm before clipping: {total_norm_before:.4f}\")\n",
        "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "total_norm_after = torch.norm(\n",
        "    torch.stack([p.grad.norm(2) for p in model.parameters() if p.grad is not None])\n",
        ").item()\n",
        "print(f\"Gradient norm after clipping:  {total_norm_after:.4f}\")\n",
        "optimizer.step()\n",
        "\n",
        "\n",
        "# END_YOUR_CODE"
      ],
      "id": "7327507fb6e803ad",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient norm before clipping: 3.8468\n",
            "Gradient norm after clipping:  0.5000\n"
          ]
        }
      ],
      "execution_count": 19
    },
    {
      "metadata": {
        "id": "9952750fa74cd487"
      },
      "cell_type": "markdown",
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:"
      ],
      "id": "9952750fa74cd487"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient clipping is used to prevent exploding gradients during training, mainly in unstable neural networks.\n",
        "\n",
        "When the gradients become too large, they can cause the model to diverge. So, clipping will make sure that the total gradient is in range.\n",
        "\n",
        "In actor-critic methods, gradient clipping is preferred when:\n",
        "The policy and value losses can create large, unbalanced updates.\n",
        "The environment has sparse or delayed rewards, which can increase the loss.\n",
        "By using clip_grad_norm_() after loss.backward(), we stop the large updates from messing up training, but still allow small changes go through. This helps the model learn more smoothly.\n"
      ],
      "metadata": {
        "id": "AGeER22ciBf6"
      },
      "id": "AGeER22ciBf6"
    },
    {
      "metadata": {
        "id": "f4cff31e6c6e7e4a"
      },
      "cell_type": "markdown",
      "source": [
        "If you are working in a team, provide a contribution summary.\n",
        "| Team Member | Step# | Contribution (%) |\n",
        "|---|---|---|\n",
        "|   | Task 1 |   |\n",
        "|   | Task 2 |   |\n",
        "|   | Task 3 |   |\n",
        "|   | Task 4 |   |\n",
        "|   | **Total** |   |\n"
      ],
      "id": "f4cff31e6c6e7e4a"
    },
    {
      "metadata": {
        "id": "4be0a6e29f281e23"
      },
      "cell_type": "code",
      "source": [],
      "id": "4be0a6e29f281e23",
      "outputs": [],
      "execution_count": 19
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}